{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Learning Objectives](#Learning-Objectives)\n",
    "\n",
    "[Background](#Background)\n",
    "\n",
    "[Rule-Based Sentiment](#Rule-Based-Sentiment-Prediction)\n",
    "\n",
    "[Test Yourself](#Test-Yourself)\n",
    "\n",
    "[Limitations](#Limitations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "Experience the full data science workflow from data aquisition, pre-processing, to building a model and presenting the results. \n",
    "\n",
    "* Work with free-form text data.\n",
    "* Learn and understand two approches to sentiment analysis.\n",
    "* Get to know how to use an APIs to scrape Twitter data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Today we want to look at ways to help buisness make the most out of their customers feedback, which oftentimes comes as textual reviews or comments. Sentiment Analysis, or categorizing attitudes towards something, is quite relevant today. Amazon, for example, sells products of all sorts; those who purchase these items are able to leave reviews and comments. Besides the ratings that are given, how would a company be able to tell which products are well-liked and which ones should be removed?\n",
    "\n",
    "An easy way is through sentiment analysis, where the goal is to predict the sentiment or positivity/negativity of a product or service solely based on the text provided as comments and reviews. Throughout this lab, we will explore two different ways to predict and understand the sentiment of text data. First, we will work through a simple rule-based algorithm, looking at positive and negative words to determine the classification of reviews. Following this, we will work through a more sophisticated machine learning based approach, allowing us to scale our classification to much larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-Based Sentiment Prediction\n",
    "\n",
    "Rule-based sentiment prediction is the easier of the two algorithms to learn and implement. In short, we have a list of positive words and a list of negative words, both of which will be used to calculate a \"sentiment score\" for the review.\n",
    "\n",
    "For example, let's say we have two sets of words, positive_words and negative_words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = ['great', 'awesome', 'happy', 'good', 'exciting', 'love']\n",
    "negative_words = ['bad', 'dislike', 'sad', 'boring', 'awful', 'poor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have a set of reviews or text that we want to analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['I thought the movie was great! I was very happy I could see it.',\n",
    "           'I did not like the movie; boring acting, poor attitudes, bad lighting.',\n",
    "           'The movie was pretty exciting overall, but the sound quality was bad.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then go through each review and add or subtract to the sentiment score based on the number of positive or negative words. If there is a positive word, then we add one to the score; a negative word subtracts one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = []\n",
    "for review in reviews:\n",
    "    sentiment_score = 0\n",
    "    for word in review.split(' '):\n",
    "        if word in positive_words:\n",
    "            sentiment_score += 1\n",
    "        if word in negative_words:\n",
    "            sentiment_score -= 1\n",
    "    sentiment_scores.append(sentiment_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out these results to see the overall scores in order of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, -3, 1]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do this by hand, we see that the scores don't add up correctly. Why is this? The words of the reviews are split by spaces. Take the first review for example. If we split it by spaces and look at the words, we see that the word great still has the exclamation point with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'thought', 'the', 'show', 'was', 'great!', 'I', 'was', 'very', 'happy', 'I', 'could', 'see', 'it.']\n"
     ]
    }
   ],
   "source": [
    "first_review = 'I thought the show was great! I was very happy I could see it.'\n",
    "first_review_words = first_review.split(' ')\n",
    "print(first_review_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the words split only by spaces causes some words to include punctuation, which is something we don't want. We won't touch on this too much, but preprocessing data to make sure words or numbers are functioning correctly can increase performance and accuracy greatly. Making sure that punctuation is removed as well as standardizing to lowercase gives much more control over the text data at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'thought', 'the', 'show', 'was', 'great', 'i', 'was', 'very', 'happy', 'i', 'could', 'see', 'it']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# .lower() changes first_review to all lowercase\n",
    "# .translate(str.maketrans(input, output, delete)) will replace characters from input with respective \n",
    "#      characters in output and deletes what's in delete. \n",
    "#      --> for example: translate(str.maketrans(“aeiou”, “12345\", \"!\")) will replace vowels with their respective \n",
    "#          numbers and deletes all exclamation marks\n",
    "# .split(' ') splits the words into an array based on ' ', or a space\n",
    "# other functions include:\n",
    "# .replace(target, new), which will replace all matches of the target string with the new string\n",
    "\n",
    "new_first_words = first_review.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).split(\" \")\n",
    "print(new_first_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now re-run this code on the reviews to see the appropriate scores that should be allocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, -3, 0]\n"
     ]
    }
   ],
   "source": [
    "sentiment_scores = []\n",
    "for review in reviews:\n",
    "    sentiment_score = 0\n",
    "    for word in review.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).split(\" \"):\n",
    "        if word in positive_words:\n",
    "            sentiment_score += 1\n",
    "        if word in negative_words:\n",
    "            sentiment_score -= 1\n",
    "    sentiment_scores.append(sentiment_score)\n",
    "\n",
    "print(sentiment_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We now have a working function to assign sentiment scores to reviews. The final step is simply to assign a sentiment to the reviews. There are several ways to approach this, depending on what the user is attempting to do. We could work this as a Binary Classification, where each review is either positive or negative, and cannot be anything else. For this, we would assign \"Negative\" to any review with a score less than zero, and \"Positive\" to every other review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive', 'Negative', 'Positive']\n"
     ]
    }
   ],
   "source": [
    "review_sentiments = []\n",
    "\n",
    "for score in sentiment_scores:\n",
    "    if score >= 0:\n",
    "        review_sentiments.append(\"Positive\")\n",
    "    if score < 0:\n",
    "        review_sentiments.append(\"Negative\")\n",
    "        \n",
    "print(review_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we could also use Multi-class classification, including a \"Neutral\" class for the reviews that have a score of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive', 'Negative', 'Neutral']\n"
     ]
    }
   ],
   "source": [
    "review_sentiments = []\n",
    "\n",
    "for score in sentiment_scores:\n",
    "    if score > 0:\n",
    "        review_sentiments.append(\"Positive\")\n",
    "    if score < 0:\n",
    "        review_sentiments.append(\"Negative\")\n",
    "    if score == 0:\n",
    "        review_sentiments.append(\"Neutral\")\n",
    "        \n",
    "print(review_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all of this in mind, there are no limits to the number of classes or splits that could be made for text data. We could adjust the range for neutral to be any reviews between -1 and 1, or perhaps add in more classes (\"Slightly Positive\", \"Slightly Negative\", \"Very Positive\", \"Very Negative\", etc...). As long as the data is preprocessed correctly and you have a good set of positive and negative words, you will be able to run sentiment analysis on the majority of text files.\n",
    "\n",
    "### Caution!\n",
    "\n",
    "Though Rule-Based Sentiment Analysis is quick and on the easier side to implement, there are several drawbacks that can render this method inefficient. This method does not take into account misspellings, nor does it take into account context. Take the two following reviews for example.\n",
    "\n",
    "\"The movie was not good, it was bad\" and \"The movie was not bad, it was good\". \n",
    "\n",
    "Both of these reviews would end up with the same sentiment score, but are clearly different reviews. This is partly due to the nature of the method; we are only looking at one word at a time, and not pairs of words. We will not look at this specifically, but looking at pairs of words or groups of three word (called bi-grams or tri-grams or in general n-grams) can help alleviate mistakes in our analysis.\n",
    "\n",
    "Rule-Based Sentiment Analysis also does not take into account the length of the review. If we have a very long review that uses a mix of positive and negative words, it may end up being classified as something it is not. Likewise, a short but very strongly opinionated review may not receive the same sentiment as a longer, equally opinionated review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Yourself\n",
    "\n",
    "In the following code blocks, work through to analyze **real-life movie reviews**! Some of the code is written for you, some will have to be filled in yourself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped Negative\n",
      "Unzipped Positive\n",
      "Created list of negtaive words: negative_words\n",
      "Created list of postive words: postitive_words\n"
     ]
    }
   ],
   "source": [
    "# Setup - This cell block is needed to set up everything for this testing section\n",
    "# No need to edit this cell\n",
    "\n",
    "import os\n",
    "import string\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists('utility/data/neg'):\n",
    "    zip_ref = zipfile.ZipFile('utility/data/neg.zip', 'r')\n",
    "    zip_ref.extractall('utility/data/')\n",
    "    zip_ref.close()\n",
    "    print('Unzipped Negative')\n",
    "\n",
    "\n",
    "if not os.path.exists('utility/data/pos'):\n",
    "    zip_ref = zipfile.ZipFile('utility/data/pos.zip', 'r')\n",
    "    zip_ref.extractall('utility/data/')\n",
    "    zip_ref.close()\n",
    "    print('Unzipped Positive')\n",
    "    \n",
    "with open('utility/data/negative-words.txt') as f:\n",
    "    negative_words = [word.strip() for word in f.readlines() if word[0] not in [';', '\\n']]\n",
    "    print('Created list of negtaive words: negative_words')\n",
    "\n",
    "with open('utility/data/positive-words.txt') as f:\n",
    "    positive_words = [word.strip() for word in f.readlines() if word[0] not in [';', '\\n']]\n",
    "    print('Created list of postive words: postitive_words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bulk of the code will be executed in the following cell. Fill in what needs to be filled in to perform rule-based sentiment prediction on the **_postive reviews_**! Running this cell will take a little while as it needs to go through all the reviews and count the postivie and negative words in oder to get the sentiment score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Running\n"
     ]
    }
   ],
   "source": [
    "# Create a blank sentiment_scores list\n",
    "import csv\n",
    "\n",
    "\n",
    "sentiment_scores = []\n",
    "\n",
    "# Create a variable called file_path for the data folder\n",
    "# Hint: Data is stored in the folder 'data', with two subfolders being 'neg' or 'pos'\n",
    "\n",
    "file_path = \"utility/data/pos\"\n",
    "\n",
    "for file in os.listdir(file_path):\n",
    "    file_start = file_path + '/'\n",
    "    \n",
    "    # Create a sentiment_score variable for this review, and set it to zero\n",
    "    sentiment_score = 0\n",
    "        \n",
    "    with open(file_start + file, encoding=\"utf8\") as f:\n",
    "\n",
    "        # Pull the words into a words array\n",
    "        # The reviews include the string \\\"<br />\\\" quite a few times; the data looks cleaner if replaced\n",
    "        # with a space!\n",
    "\n",
    "        my_words = f.read()\n",
    "        my_words = my_words.replace('<br />', ' ')\n",
    "\n",
    "        # Hint: Remember to read, lower, replace, translate, and split!\n",
    "        words = my_words.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).split(\" \")\n",
    "        \n",
    "        # Loop through the words to generate the sentiment score\n",
    "        \n",
    "        for word in words:\n",
    "                if word in positive_words:\n",
    "                        sentiment_score += 1\n",
    "                if word in negative_words:\n",
    "                        sentiment_score -= 1\n",
    "                \n",
    "       \n",
    "        # Append the sentiment_score to the sentiment_scores array!\n",
    "        sentiment_scores.append(sentiment_score)\n",
    "\n",
    "                                \n",
    "\n",
    "print('Done Running')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Analyzing the Results\n",
    "\n",
    "Now, we can see how our apporach predicts the sentiment for those reviews. This phase is a crucial part in the data science workflow and is called model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.40% positive reviews\n",
      "15.60% negative reviews\n"
     ]
    }
   ],
   "source": [
    "# Positive Reviews:\n",
    "percent_pos = sum([1 for score in sentiment_scores if score >= 0]) / len(sentiment_scores)*100\n",
    "print(\"%.2f%% positive reviews\" % (percent_pos))\n",
    "positive_percent_pos = percent_pos\n",
    "\n",
    "# Negative Reviews:\n",
    "percent_neg = sum([1 for score in sentiment_scores if score < 0]) / len(sentiment_scores)*100\n",
    "print(\"%.2f%% negative reviews\" % (percent_neg))\n",
    "positive_percent_neg = percent_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these number mean? Explain whether our approach works well or not.\n",
    "\n",
    "84.30% positive reviews means that in the pos folder where we have our positive examples, our classifier was able to detect 84.30% of them as positively correctly. \n",
    "15.70% negative reviews means that from the pos folder, where we have our positive examples, our classifier has detected 15.70% files as wrongly negative.\n",
    "\n",
    "In my opinion, this approach is working decent since it is able to classify more than 80% of our positive files properly under the positive classification. However, some files are wrongly predicted as negative because of this approach's inability to take into account the context of the review and because it assigns sentiment score based on single words, it regards feedback like 'not bad' as a negative sentiment. Hence, it is a decent approach with a good scope of errors.\n",
    "\n",
    "Repeat the above computations for the **_negative reviews_** and compare the results with the ones above. Is our approach better in predictiong postivie reviews correctly or negative ones?\n",
    "\n",
    "When the negative analysis is done, we see that the classifier was able to classify 72.75% of the negative files properly under the negative classification ubmbrella. However, it went wrong with 27.35% of reviews and incorrectly classified them as positive. \n",
    "Since this % of correct negative classification is lesser when compared to the % of correct positive classification we got while running over the postive reviews, we can conclude that this approach worked better in positive review predictions as compared to the negative review predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Running\n",
      "27.30% positive reviews\n",
      "72.70% negative reviews\n"
     ]
    }
   ],
   "source": [
    "# Create a blank sentiment_scores list\n",
    "import csv\n",
    "\n",
    "\n",
    "sentiment_scores = []\n",
    "\n",
    "# Create a variable called file_path for the data folder\n",
    "# Hint: Data is stored in the folder 'data', with two subfolders being 'neg' or 'pos'\n",
    "\n",
    "file_path = \"utility/data/neg\"\n",
    "\n",
    "for file in os.listdir(file_path):\n",
    "    file_start = file_path + '/'\n",
    "    \n",
    "    # Create a sentiment_score variable for this review, and set it to zero\n",
    "    sentiment_score = 0\n",
    "    \n",
    "    with open(file_start + file, encoding=\"utf8\") as f:\n",
    "\n",
    "        # Pull the words into a words array\n",
    "        my_words = f.read()\n",
    "        my_words = my_words.replace('<br />', ' ')\n",
    "\n",
    "        words = my_words.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).split(\" \")\n",
    "        \n",
    "        # Loop through the words to generate the sentiment score\n",
    "        for word in words:\n",
    "                if word in positive_words:\n",
    "                        sentiment_score += 1\n",
    "                if word in negative_words:\n",
    "                        sentiment_score -= 1\n",
    "                \n",
    "       \n",
    "        # Append the sentiment_score to the sentiment_scores array!\n",
    "        sentiment_scores.append(sentiment_score)                          \n",
    "\n",
    "print('Done Running')\n",
    "\n",
    "percent_pos = sum([1 for score in sentiment_scores if score >= 0]) / len(sentiment_scores)*100\n",
    "print(\"%.2f%% positive reviews\" % (percent_pos))\n",
    "negative_percent_pos = percent_pos\n",
    "\n",
    "# Negative Reviews:\n",
    "percent_neg = sum([1 for score in sentiment_scores if score < 0]) / len(sentiment_scores)*100\n",
    "print(\"%.2f%% negative reviews\" % (percent_neg))\n",
    "negative_percent_neg = percent_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the allowver performance of our rule-based sentiment perdictor? Compute the percentage of correctly predicted reviews (this measure is allso called _accuracy_) and the percentage of incorrectly predicted reviews (this measure is also called _error rate_). Hint: both measures should add up to 100%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  78.55\n",
      "Error rate  21.45\n"
     ]
    }
   ],
   "source": [
    "accuracy = (positive_percent_pos + negative_percent_neg)/2\n",
    "print(\"Accuracy \",accuracy)\n",
    "error = (positive_percent_neg + negative_percent_pos)/2\n",
    "print(\"Error rate \",error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations and Introduction to Machine Learning\n",
    "The rule-based sentiment predictor has many advantages including that is so simple to implement. With just a couple of extensions to our version (such as negation handling) we could actually make this production ready. However, the main drawback of this apporach is that we need **hand engineered** lists of positive and negative expressions, which are non-trivial to create and also static. That means they don't adapt automatically to the domain they are being used for. For example, in formal language expressions might have different meaning than in a colloquial context.\n",
    "\n",
    "How can we overcome this problem? Can we maybe learn what expressions are used in a possitive versus a negative review? The answer is '_yes - we can!_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Introduction to Sentiment Classification\n",
    "\n",
    "Instead of working with lists of positive and negative expressions we will now look at reviews with known ratings and use thoese to learn what positive versus negative reviews are. With those known set of positive and negative reviews, we can build a model.\n",
    "\n",
    "\n",
    "And then use it on new comments and reviews to determine a customer's attitudes. This approach is a **machine learning** approach commonly know as _classification_.\n",
    "\n",
    "Okay, let's do it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 4000\n",
      "Number of words: 8870\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_folder = \"utility/data/\"\n",
    "dataset = load_files(data_folder, shuffle=False)\n",
    "docs_raw = dataset.data\n",
    "\n",
    "## Text preprocessing\n",
    "docs_all = []\n",
    "for doc in docs_raw:\n",
    "    docs_all.append(doc.decode('utf-8', errors='replace')) # prevent UnicodeDecodeError\n",
    "y_all = dataset.target\n",
    "\n",
    "# Text tokenizing and filtering of stopwords\n",
    "count_vect = CountVectorizer(min_df=5)  \n",
    "X_all_counts = count_vect.fit_transform(docs_all)\n",
    "\n",
    "# number of docs and number of words\n",
    "print(\"Number of documents: \" + str(X_all_counts.shape[0])) \n",
    "print(\"Number of words: \" + str(X_all_counts.shape[1])) \n",
    "    # X_all_counts data representation (* = occurrence count):\n",
    "    #    - - - - -\n",
    "    #  |\n",
    "    #  |  *        <- document\n",
    "    #  |\n",
    "    #  |\n",
    "    #     ^\n",
    "    #    word index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After **preprocessing** the text documents, we **split** our data into two parts one for building the model (_training set_) \n",
    "and one for testing/evaluating it (_test/evaluation set_). Then we will **build the model** using the _training set_ and use the model to **predict** the sentiment of the documents in the _testing set_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set: 3200\n",
      "Size of the test/evaluation set: 800\n"
     ]
    }
   ],
   "source": [
    "# Split the data into two parts \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all_counts, y_all, train_size = .8, test_size = .2, random_state = 16)\n",
    "\n",
    "print(\"Size of the training set: \" + str(X_train.shape[0]))\n",
    "print(\"Size of the test/evaluation set: \" + str(X_test.shape[0]))\n",
    "\n",
    "#Build the model using a linear classification model\n",
    "model = LogisticRegression(max_iter = 10000).fit(X_train,y_train)\n",
    "\n",
    "#Use the classification model for predictions\n",
    "predicted_target = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Task\n",
    "Write a function that will go through all the test data and compare the predicted class and the actual class. If an entry is put into the wrong class by the model, this function will add one to the the respective variable: fneg_error_count if it was a _false negative_, fpos_error_count if it is a _false positive_. From these values you can compute the total _number of mistakes made_, the _error rate_ and _accuracy_ of the machine learning approach. \n",
    "\n",
    "\n",
    "Then, this function will print out how many total errors, how many false negatives, and how many false positives. The two parameters for this function are the predicted classification for each review generated by the model and the actual classification from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predictions(predictions, actual):\n",
    "    \n",
    "    fneg_error_count = 0\n",
    "    fpos_error_count = 0\n",
    "    mistakes = 0\n",
    "    error_rate = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    for idx, x in enumerate(predictions):\n",
    "        if(x != actual[idx] and x == 0):\n",
    "            fneg_error_count = fneg_error_count + 1\n",
    "            mistakes = mistakes + 1\n",
    "        elif (x!= actual[idx] and x == 1):\n",
    "            fpos_error_count = fpos_error_count + 1\n",
    "            mistakes = mistakes + 1\n",
    "    \n",
    "    error_rate = (mistakes /len(predictions)) * 100\n",
    "    accuracy = 100 - error_rate\n",
    " \n",
    "    print(\"There were a total of \" + str(mistakes) + \" errors out of \" + str(len(predictions)) + \" testpoints.\")\n",
    "    print(\"There were \" + str(fneg_error_count) + \" false negative errors\")\n",
    "    print(\"There were \" + str(fpos_error_count) + \" false positive errors\")\n",
    "    print(\"The algorithm was wrong in \" + str(error_rate) + \"% of the test cases.\" )\n",
    "    print(\"The algorithm was correct in \" + str(accuracy) + \"% of the test cases.\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Sentiment Classifier\n",
    "Now, we can call this function using our predicted sentiments and the ground truth sentiments as input: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were a total of 77 errors out of 800 testpoints.\n",
      "There were 30 false negative errors\n",
      "There were 47 false positive errors\n",
      "The algorithm was wrong in 9.625% of the test cases.\n",
      "The algorithm was correct in 90.375% of the test cases.\n"
     ]
    }
   ],
   "source": [
    "test_predictions(predicted_target, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to try\n",
    "* Play with the train/test split sizes. we used a 80/20 split, but you can change this and see if it has an effect on the results. \n",
    "* Play with the random seed, to creat differnt train/test splits. How does this affect the results? \n",
    "* Use a differnt classifier, for example, NaiveBayes or a Support Vector Machine (SVM). Code examples are below - replace the model computation in the cell above with the respective lines to compute these differnt models. Do these models produce different results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC(max_iter = 10000).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set: 2400\n",
      "Size of the test/evaluation set: 1600\n",
      "There were a total of 163 errors out of 1600 testpoints.\n",
      "There were 68 false negative errors\n",
      "There were 95 false positive errors\n",
      "The algorithm was wrong in 10.1875% of the test cases.\n",
      "The algorithm was correct in 89.8125% of the test cases.\n",
      "Size of the training set: 3600\n",
      "Size of the test/evaluation set: 400\n",
      "There were a total of 39 errors out of 400 testpoints.\n",
      "There were 14 false negative errors\n",
      "There were 25 false positive errors\n",
      "The algorithm was wrong in 9.75% of the test cases.\n",
      "The algorithm was correct in 90.25% of the test cases.\n"
     ]
    }
   ],
   "source": [
    "# USING DIFFERENT TRAIN/TEST SPLIT SIZE\n",
    "\n",
    "# Use case 1 : train/test split size is 60/40 \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all_counts, y_all, train_size = .6, test_size = .4, random_state = 16)\n",
    "\n",
    "print(\"Size of the training set: \" + str(X_train.shape[0]))\n",
    "print(\"Size of the test/evaluation set: \" + str(X_test.shape[0]))\n",
    "\n",
    "#Build the model using a linear classification model\n",
    "model = LogisticRegression(max_iter = 10000).fit(X_train,y_train)\n",
    "\n",
    "#Use the classification model for predictions\n",
    "predicted_target = model.predict(X_test)\n",
    "\n",
    "test_predictions(predicted_target, y_test)\n",
    "\n",
    "\n",
    "# Use case 2 : train/test split size is 90/10\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all_counts, y_all, train_size = .9, test_size = .1, random_state = 16)\n",
    "\n",
    "print(\"Size of the training set: \" + str(X_train.shape[0]))\n",
    "print(\"Size of the test/evaluation set: \" + str(X_test.shape[0]))\n",
    "\n",
    "#Build the model using a linear classification model\n",
    "model = LogisticRegression(max_iter = 10000).fit(X_train,y_train)\n",
    "\n",
    "#Use the classification model for predictions\n",
    "predicted_target = model.predict(X_test)\n",
    "\n",
    "test_predictions(predicted_target, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, the accuracy of predicitons have a direct correlation with the size of the training data set. When we have a larger training data split, we have more accurate predictions. We can observe above that when the split ratio of train/test is 60/40, the accuracy is worse than when train/test was 80/20. Similarly, at the same time, the accuracy of train/test at 90/10 is better than when it is run at 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set: 3200\n",
      "Size of the test/evaluation set: 800\n",
      "There were a total of 67 errors out of 800 testpoints.\n",
      "There were 30 false negative errors\n",
      "There were 37 false positive errors\n",
      "The algorithm was wrong in 8.375% of the test cases.\n",
      "The algorithm was correct in 91.625% of the test cases.\n"
     ]
    }
   ],
   "source": [
    "# USING DIFFERENT RANDOM SEED\n",
    "\n",
    "# Split the data into two parts \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all_counts, y_all, train_size = .8, test_size = .2, random_state = 4)\n",
    "\n",
    "print(\"Size of the training set: \" + str(X_train.shape[0]))\n",
    "print(\"Size of the test/evaluation set: \" + str(X_test.shape[0]))\n",
    "\n",
    "#Build the model using a linear classification model\n",
    "model = LogisticRegression(max_iter = 10000).fit(X_train,y_train)\n",
    "\n",
    "#Use the classification model for predictions\n",
    "predicted_target = model.predict(X_test)\n",
    "test_predictions(predicted_target, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random state (a model hyperparameter) is used to regulate the unpredictability present in machine learning models. When we set the random state as a different Integer value, the result comes out to be different because we get different train and test sets with different integer values for random_state.\n",
    "\n",
    "In the above example, when random set was 16 we got the accuracy rate at 90.25% whereas when we selected the random set as 4, we got the accuracy rate as 91.625%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set: 3200\n",
      "Size of the test/evaluation set: 800\n",
      "There were a total of 75 errors out of 800 testpoints.\n",
      "There were 50 false negative errors\n",
      "There were 25 false positive errors\n",
      "The algorithm was wrong in 9.375% of the test cases.\n",
      "The algorithm was correct in 90.625% of the test cases.\n",
      "Size of the training set: 3200\n",
      "Size of the test/evaluation set: 800\n",
      "There were a total of 79 errors out of 800 testpoints.\n",
      "There were 29 false negative errors\n",
      "There were 50 false positive errors\n",
      "The algorithm was wrong in 9.875% of the test cases.\n",
      "The algorithm was correct in 90.125% of the test cases.\n"
     ]
    }
   ],
   "source": [
    "# USING DIFFERENT CLASSIFIER\n",
    "\n",
    "# Split the data into two parts \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all_counts, y_all, train_size = .8, test_size = .2, random_state = 16)\n",
    "\n",
    "print(\"Size of the training set: \" + str(X_train.shape[0]))\n",
    "print(\"Size of the test/evaluation set: \" + str(X_test.shape[0]))\n",
    "\n",
    "#Build the model using a NaiveBayes model\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "#Use the classification model for predictions\n",
    "predicted_target = model.predict(X_test)\n",
    "test_predictions(predicted_target, y_test)\n",
    "\n",
    "\n",
    "# Split the data into two parts \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all_counts, y_all, train_size = .8, test_size = .2, random_state = 16)\n",
    "\n",
    "print(\"Size of the training set: \" + str(X_train.shape[0]))\n",
    "print(\"Size of the test/evaluation set: \" + str(X_test.shape[0]))\n",
    "\n",
    "#Build the model using Linear SVC\n",
    "model = LinearSVC(max_iter = 10000).fit(X_train,y_train)\n",
    "\n",
    "#Use the classification model for predictions\n",
    "predicted_target = model.predict(X_test)\n",
    "test_predictions(predicted_target, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see,\n",
    "\n",
    "MultinomialNB : algorithm was correct in 90.625% of the test cases\n",
    "\n",
    "LinearSVC : algorithm was correct in 90.125% of the test cases\n",
    "\n",
    "LogisticRegression : algorithm was correct in 91.625% of the test cases\n",
    "\n",
    "All the three models generate different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it turns out that this performs quite well. Of course, we can do more fancy things with the text data, instead of only counting word occurrences. In practice, people use for instance the counts of _pairs of words_ (so-called _bi-grams_) or even _n-grams_ (counts of tuples of n words), or a feature called _TF-IDF_, which is very powerful in practice. This is a nice tutorial explaining how to compute those: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Compare the two apporaches **rule-based sentiment prediction** versus **sentiment classification**. What are the main differences in terms of... \n",
    "* required data?\n",
    "\n",
    "Rule based sentiment analysis will require a hand-engineered list of categorized words (for eg. a list of positive and negative words in our case) in order to predict the sentiment.\n",
    "On the other hand, sentiment classification requires some already classified set of data for creating the training data sets. This training set is used in the machine learning model under the sentiment classification approach. In order to be properly trained and make accurate predictions, the machine learning model requires a lot of data.\n",
    "\n",
    "* quality of the results?\n",
    "\n",
    "The quality of results are generally better in sentiment classification as compared to rule-based sentiment prediction for several reasons. One of the main reasons is that the rule-based classification does not adapt automatically to the domain they are being used for. Which means that some terms in one formal language can have a different meaning in say a colloquial language. Another reason would be due to the static list of hand-engineered list of rules. Since the words referred to for classification are already pre-defined, any new word encountered which is not a part of any of the lists would be dis-regarded. On the other hand, sentiment classification adopts to the domain of the training data sets and is more efficient when making predicitons for the test data sets. Also, since it learns from the words that are a part of the training data sets, probability of accurate predictions increases.\n",
    "\n",
    "* efficiency of the computation?\n",
    "\n",
    "In the cases where the volume of the data is low and rules are relatively simple, rule based classification can be more efficient in terms of computation and time as compared to sentiment classification. But in other cases i.e when we have large volumes of data, definitely the efficiency of computation of sentiment classification will be better as compared to rule based classification.\n",
    "\n",
    "* possibilities to extend the basic algorithms? \n",
    "\n",
    "The basic alogorithms can definitely be extended to be improved in some ways. One way would be to add conditions like use n-grams to understand the sentiment of a sentence better when it has a mix of both positive and negative connotation.  For eg. If a sentence is - 'I do not like coffee': this will have one positive word - 'like' and one negative word - 'not' which will lead the sentiment score to become 0 i.e neutral. Now clearly, we know that this sentence has a negative connotation but the classifier will classify it as neutral which is not correct. But if we use bi-grams then 'not like' can be correctly classified as negative instead of neutral. Another way the alogrithm can be extended it by including varying levels of sentiments like 'positive', 'highly positive' 'negative', 'highly negative', 'neutral'. This will allow the data to be classified more accurately and will be closer to the real sentiment.\n",
    "\n",
    "Make a list of _pros_ and _cons_ for both approaches and also think of use cases/applications for either technique. \n",
    "\n",
    "Rule-based Prediction:\n",
    "\n",
    "pros : \n",
    "    1) Very easy to implement\n",
    "    2) Training data not required\n",
    "    3) A good technique to gather data since one can set up the system with rules and let data accumulate spontaneously when users interact with it. \n",
    "\n",
    "cons :\n",
    "    1) Does not take into account the account context/domain nor misspellings\n",
    "    2) Does not take into account the length of the review. It can classify incorrectly (for eg. if there is a long review which is a mix of positive and negative words, it can go wrong) and also in the case of rule-based sentiment classification, short strong reviews do not hold the same sentiment as long strong reviews.\n",
    "    3) When there is huge amounts of data, this approach can become generally in-efficient.\n",
    "\n",
    "use case : In cold start scenarios i.e many a times when we have the cold start problem (No data to begin with) in Machine Learning, rule based approach make sense as it does not require any training data to begin with. This approach is also used when there is low volume of data and we need to make predictions.\n",
    "\n",
    "\n",
    "Sentiment classification:\n",
    "\n",
    "pros : \n",
    "    1) Does not require pre-defined hand-engineered list of rules\n",
    "    2) Takes into account domain of text and is dynamic based on the training data.\n",
    "    3) Useful for large amount of data\n",
    "\n",
    "cons :\n",
    "    1) Adaptability and speed of machine learning systems come at a cost\n",
    "    2) Training data set is required\n",
    "    3) In order for the models to improve on accuracy of predictions, the machine learning algorithm in sentiment classification requires a lot of data\n",
    "\n",
    "use case: This approach is highly suitable when we have to handle complex and intensive issues with a relatively variable environment/domain. It is also particularly useful for high-volume use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up\n",
    "Please run the following cell in order to clean up some of the files on your computer. While not mandatory, it will certainly save some space (over 4000 files are already unzipped, this will clear space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to clean folders (unless you want to keep several thousand text files on your computer!)\n",
    "\n",
    "if os.path.exists('utility/data/neg'):\n",
    "    shutil.rmtree('utility/data/neg')\n",
    "if os.path.exists('utility/data/pos'):\n",
    "    shutil.rmtree('utility/data/pos')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "19aafbfcbbb9baf57e7c243a30b74d68d5b97b74b8628c69e27a3c00302d2378"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
